{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src = \"https://ibm.box.com/shared/static/wbqvbi6o6ip0vz55ua5gp17g4f1k7ve9.png\" width = 300, align = \"center\"></a>\n",
    "\n",
    "<h1 align=center><font size = 5>RECOMMENDATION SYSTEM WITH A RESTRICTED BOLTZMANN MACHINE</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the **Recommendation System with a Restricted Boltzmann Machine** notebook. In this notebook, we study and go over the usage of a Restricted Boltzmann Machine (RBM) in a Collaborative Filtering based recommendation system. This system is an algorithm that recommends items by trying to find users that are similar to each other based on their item ratings. By the end of this notebook, you should have a deeper understanding of how Restricted Boltzmann Machines are applied, and how to build one using TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "- <p><a href=\"#ref1\">Acquiring the Data</a></p>\n",
    "- <p><a href=\"#ref2\">Loading in the Data</a></p>\n",
    "- <p><a href=\"#ref3\">The Restricted Boltzmann Machine model</a></p>\n",
    "- <p><a href=\"#ref4\">Setting the Model's Parameters</a></p>\n",
    "- <p><a href=\"#ref1337\">Recommendation</a></p>\n",
    "<p></p>\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1\"></a>\n",
    "# Acquiring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we need to download the data we are going to use for our system. The datasets we're going to use were acquired by [GroupLens](http://grouplens.org/datasets/movielens/) and contain movies, users and movie ratings by these users.\n",
    "\n",
    "After the download is done, we extract the datasets to a directory that's easily accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O moviedataset.zip http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
    "!unzip -o moviedataset.zip -d /resources/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With the datasets in place, let's now import the necessary libraries. We will be using [Tensorflow](https://www.tensorflow.org/) and [Numpy](http://www.numpy.org/) together to model and initialize our Restricted Boltzmann Machine and [Pandas](http://pandas.pydata.org/pandas-docs/stable/) to manipulate our datasets. To import these libraries, run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tensorflow library. Used to implement machine learning models\n",
    "import tensorflow as tf\n",
    "#Numpy contains helpful functions for efficient mathematical calculations\n",
    "import numpy as np\n",
    "#Dataframe manipulation library\n",
    "import pandas as pd\n",
    "#Graph plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref2\"></a>\n",
    "# Loading in the Data\n",
    "\n",
    "Let's begin by loading in our data with Pandas. The .dat files containing our data are similar to CSV files, but instead of using the ',' (comma) character to separate entries, it uses '::' (two colons) characters instead. To let Pandas know that it should separate data points at every '::', we have to specify the `sep='::'` parameter when calling the function.\n",
    "\n",
    "Additionally, we also pass it the `header=None` parameter due to the fact that our files don't contain any headers.\n",
    "\n",
    "Let's start with the movies.dat file and take a look at its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading in the movies dataset\n",
    "movies_df = pd.read_csv('/resources/data/ml-1m/movies.dat', sep='::', header=None)\n",
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for the ratings.dat file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Loading in the ratings dataset\n",
    "ratings_df = pd.read_csv('/resources/data/ml-1m/ratings.dat', sep='::', header=None)\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our movies_df variable contains a dataframe that stores a movie's unique ID number, title and genres, while our ratings_df variable stores a unique User ID number, a movie's ID that the user has watched, the user's rating to said movie and when the user rated that movie.\n",
    "\n",
    "Let's now rename the columns in these dataframes so we can better convey their data more intuitively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies_df.columns = ['MovieID', 'Title', 'Genres']\n",
    "ratings_df.columns = ['UserID', 'MovieID', 'Rating', 'Timestamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our final movies_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our final ratings_df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "# The Restricted Boltzmann Machine model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ibm.box.com/shared/static/o049tx0dsllpbj3b546vuba25qqlzelq.png\" alt=\"RBM Model\" style=\"width: 300px;\"/>\n",
    "\n",
    "The Restricted Boltzmann Machine model has two layers of neurons, one of which is what we call a visible input layer and the other is called a hidden layer. The hidden layer is used to learn features from the information fed through the input layer. For our model, the input is going to contain X neurons, where X is the amount of movies in our dataset. Each of these neurons will possess a normalized rating value varying from 0 to 1 -- 0 meaning that a user has not watched that movie and the closer the value is to 1, the more the user likes the movie that neuron's representing. These normalized values, of course, will be extracted and normalized from the ratings dataset.\n",
    "\n",
    "After passing in the input, we train the RBM on it and have the hidden layer learn its features. These features are what we use to reconstruct the input, which in our case, will predict the ratings for movies that the input hasn't watched, which is exactly what we can use to recommend movies!\n",
    "\n",
    "We will now begin to format our dataset to follow the model's expected input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see how many movies we have and see if the movie ID's correspond with that value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is possible to notice, we have 3883 movies, while our ID's vary from 1 to 3952. Due to this, we won't be able to index movies through their ID since we would get memory indexing errors. To amend this, we can create a column that shows what spot in our list that particular movie is in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_df['List Index'] = movies_df.index\n",
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, let's merge the ratings dataframe into the movies one so we can have the List Index values in both dataframes. Additionally we're also going to drop the Timestamp, Title and Genres columns since we won't be needing it to make recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Merging movies_df with ratings_df by MovieID\n",
    "merged_df = movies_df.merge(ratings_df, on='MovieID')\n",
    "#Dropping unecessary columns\n",
    "merged_df = merged_df.drop('Timestamp', axis=1).drop('Title', axis=1).drop('Genres', axis=1)\n",
    "#Displaying the result\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also group up the users by their user IDs and take a look at one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group up by UserID\n",
    "userGroup = merged_df.groupby('UserID')\n",
    "userGroup.first().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start formatting the data into input for the RBM. We're going to store the normalized users ratings into a list of lists called trX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Amount of users used for training\n",
    "amountOfUsedUsers = 1000\n",
    "#Creating the training list\n",
    "trX = []\n",
    "#For each user in the group\n",
    "for userID, curUser in userGroup:\n",
    "    #Create a temp that stores every movie's rating\n",
    "    temp = [0]*len(movies_df)\n",
    "    #For each movie in curUser's movie list\n",
    "    for num, movie in curUser.iterrows():\n",
    "        #Divide the rating by 5 and store it\n",
    "        temp[movie['List Index']] = movie['Rating']/5.0\n",
    "    #Now add the list of ratings into the training list\n",
    "    trX.append(temp)\n",
    "    #Check to see if we finished adding in the amount of users for training\n",
    "    if amountOfUsedUsers == 0:\n",
    "        break\n",
    "    amountOfUsedUsers -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref4\"></a>\n",
    "# Setting the Model's Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's start building our RBM with Tensorflow. We'll begin by first determining the amount of hidden layers and then creating placeholder variables for storing our visible layer biases, hidden layer biases and weights that connect the hidden layer with the visible one. We will be arbitrarily setting the amount of hidden layers to 20. You can freely set this value to any number you want since each neuron in the hidden layer will end up learning a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hiddenUnits = 20\n",
    "visibleUnits = len(movies_df)\n",
    "vb = tf.placeholder(\"float\", [visibleUnits]) #Number of unique movies\n",
    "hb = tf.placeholder(\"float\", [hiddenUnits]) #Number of features we're going to learn\n",
    "W = tf.placeholder(\"float\", [visibleUnits, hiddenUnits])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then move on to creating the visible and hidden layer units and setting their activation functions. In this case, we will be using the `tf.sigmoid` and `tf.relu` functions as nonlinear activations since it's what is usually used in RBM's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Phase 1: Input Processing\n",
    "v0 = tf.placeholder(\"float\", [None, visibleUnits])\n",
    "_h0= tf.nn.sigmoid(tf.matmul(v0, W) + hb)\n",
    "h0 = tf.nn.relu(tf.sign(_h0 - tf.random_uniform(tf.shape(_h0))))\n",
    "#Phase 2: Reconstruction\n",
    "_v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb) \n",
    "v1 = tf.nn.relu(tf.sign(_v1 - tf.random_uniform(tf.shape(_v1))))\n",
    "h1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we set the RBM training parameters and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Learning rate\n",
    "alpha = 1.0\n",
    "#Create the gradients\n",
    "w_pos_grad = tf.matmul(tf.transpose(v0), h0)\n",
    "w_neg_grad = tf.matmul(tf.transpose(v1), h1)\n",
    "#Calculate the Contrastive Divergence to maximize\n",
    "CD = (w_pos_grad - w_neg_grad) / tf.to_float(tf.shape(v0)[0])\n",
    "#Create methods to update the weights and biases\n",
    "update_w = W + alpha * CD\n",
    "update_vb = vb + alpha * tf.reduce_mean(v0 - v1, 0)\n",
    "update_hb = hb + alpha * tf.reduce_mean(h0 - h1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set the error function, which in this case will be the Mean Absolute Error Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err = v0 - v1\n",
    "err_sum = tf.reduce_mean(err * err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to initialize our variables. Thankfully, NumPy has a handy `zeros` function for this. We use it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Current weight\n",
    "cur_w = np.zeros([visibleUnits, hiddenUnits], np.float32)\n",
    "#Current visible unit biases\n",
    "cur_vb = np.zeros([visibleUnits], np.float32)\n",
    "#Current hidden unit biases\n",
    "cur_hb = np.zeros([hiddenUnits], np.float32)\n",
    "#Previous weight\n",
    "prv_w = np.zeros([visibleUnits, hiddenUnits], np.float32)\n",
    "#Previous visible unit biases\n",
    "prv_vb = np.zeros([visibleUnits], np.float32)\n",
    "#Previous hidden unit biases\n",
    "prv_hb = np.zeros([hiddenUnits], np.float32)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the RBM with 15 epochs with each epoch using 10 batches with size 100. After training, we print out a graph with the error by epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batchsize = 100\n",
    "errors = []\n",
    "for i in range(epochs):\n",
    "    for start, end in zip( range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n",
    "        batch = trX[start:end]\n",
    "        cur_w = sess.run(update_w, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n",
    "        cur_vb = sess.run(update_vb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n",
    "        cur_nb = sess.run(update_hb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n",
    "        prv_w = cur_w\n",
    "        prv_vb = cur_vb\n",
    "        prv_hb = cur_nb\n",
    "    errors.append(sess.run(err_sum, feed_dict={v0: trX, W: cur_w, vb: cur_vb, hb: cur_nb}))\n",
    "    print errors[-1]\n",
    "plt.plot(errors)\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1337\"></a>\n",
    "## Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict movies that an arbitrarily selected user might like. This can be accomplished by feeding in the user's watched movie preferences into the RBM and then reconstructing the input. The values that the RBM gives us will attempt to estimate the user's preferences for movies that he hasn't watched based on the preferences of the users that the RBM was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Selecting the input user\n",
    "inputUser = [trX[75]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feeding in the user and reconstructing the input\n",
    "hh0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)\n",
    "vv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\n",
    "feed = sess.run(hh0, feed_dict={ v0: inputUser, W: prv_w, hb: prv_hb})\n",
    "rec = sess.run(vv1, feed_dict={ hh0: feed, W: prv_w, vb: prv_vb})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then list the 20 most recommended movies for our mock user by sorting it by their scores given by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_movies_df_75 = movies_df\n",
    "scored_movies_df_75[\"Recommendation Score\"] = rec[0]\n",
    "scored_movies_df_75.sort([\"Recommendation Score\"], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how to recommend the movies that the user has not watched yet? \n",
    "\n",
    "Lets first find the __User ID__ of our mock user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.iloc[75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can find all the movies that our mock user has watched before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df_75 = merged_df[merged_df['UserID']==215]\n",
    "movies_df_75.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we merge all the movies that our mock users has watched with the predicted scors based on his historical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merging movies_df with ratings_df by MovieID\n",
    "merged_df_75 = scored_movies_df_75.merge(movies_df_75, on='MovieID', how='outer')\n",
    "#Dropping unecessary columns\n",
    "merged_df_75 = merged_df_75.drop('List Index_y', axis=1).drop('UserID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets sort it and take a look at the firt 20 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_75.sort([\"Recommendation Score\"], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are some movies that user has not watched yet and has high score based on our model. So, we can recommend them to user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the module. If you want, you can try to change the parameters in the code -- adding more units to the hidden layer, changing the loss functions or maybe something else to see if it changes anything. Does the model perform better? Does it take longer to compute?\n",
    "\n",
    "Thank you for reading this notebook. Hopefully, you now have a little more understanding of the RBM model, its applications and how it works with TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to learn more?\n",
    "\n",
    "Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The PowerAI platform supports popular machine learning libraries and dependencies including Tensorflow, Caffe, Torch, and Theano. You can download a [free version of PowerAI](https://cocl.us/ML0120EN_PAI).\n",
    "\n",
    "Also, you can use Data Science Experience to run these notebooks faster with bigger datasets. Data Science Experience is IBM's leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, DSX enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of DSX users today with a free account at [Data Science Experience](https://cocl.us/ML0120EN_DSX)This is the end of this lesson. Hopefully, now you have a deeper and intuitive understanding regarding the LSTM model. Thank you for reading this notebook, and good luck on your studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this exercise!\n",
    "\n",
    "Notebook created by: Gabriel Garcez Barros Sousa, <a href = \"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a>, <a href = \"https://www.linkedin.com/in/franciscomagioli\">Francisco Magioli</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [Restricted Boltzmann Machines for Collaborative Filtering](http://www.cs.utoronto.ca/~hinton/absps/netflixICML.pdf)\n",
    "* <font='red'>RBM Notebook</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Copyright &copy; 2017 [IBM Cognitive Class](https://cocl.us/ML0120EN_cclab_cc). This notebook and its source code are released under the terms of the [MIT License](https://cognitiveclass.ai/mit-license/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
